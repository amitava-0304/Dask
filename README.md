# Results and Conclusion

- **Dask Processing Time**: 114.24 seconds
- **Concurrent Processing Time**: 6562.28 seconds
- **Number of Rows in `dask_results` Table**: 2,611,892

### Analysis
- **Efficiency**: Dask is significantly more efficient than concurrent processing for this ETL task.
- **Resource Utilization**: Dask optimizes resource utilization better, resulting in faster processing times.
- **Accuracy**: Both methods correctly processed and loaded the entire dataset.

### Conclusion
For large-scale ETL processes, Dask provides substantial performance benefits and should be preferred for its efficiency and scalability.

# Step-by-Step Explanation of the ETL Process
**1. Imports and Initial Setup**
<br>First, we need to import the necessary libraries and configure logging to record the ETL process steps.

**2. ETLProcess Class Initialization**
<br>We define the ETLProcess class, which initializes with a data array, batch size, database URL, and table name.

**3. Batch Processing Method**
<br>The square_and_sleep_batch method simulates processing time and multiplies the numbers in each batch.

**4. Extract Method**
<br>The extract method splits the data into batches based on the specified batch size.

**5. Transform Methods**
There are two transformation methods:
<ol><li><h5>Dask Transform:</h5>Uses Dask to process the batches concurrently.
<p align=justify>Initializes a Dask client with adjusted parameters (number of workers, threads per worker, memory limit). 
Creates a list of delayed tasks using delayed from dask. 
Each delayed task represents a call to the square_and_sleep_batch function on a specific batch of data.
These tasks are not executed yet. 
Uses compute from dask to trigger the execution of the delayed tasks in parallel across Dask workers. Records the Dask processing time and prints it. Closes the Dask client to release resources. Returns the list of results obtained from the parallel execution.</p>     
<li><h5>Concurrent Transform:</h5> Uses Python's concurrent futures to process the batches concurrently. 
<p align=justify> concurrent processing using a process pool. Records the start time for concurrent processing.
Creates a process pool with a specified number of processes (adjusted based on CPU cores using os.cpu_count() in a 
real implementation). Uses pool.map from multiprocessing to distribute the square_and_sleep_batch function calls
across the processes in the pool. Each process will handle one batch of data. Records the end time for concurrent processing 
and calculates the total processing time. Prints the concurrent processing time (including the sleep simulation).
Flattens the nested list structure of results from the pool using itertools.chain.from_iterable to obtain a single list. 
Returns the flattened list of processed results. </ol>   </p>  

**6. Load Method**
<br>The load method stores the processed data into the specified SQLite database table.

**7. Row Count Method**
<br>The row_count method counts the number of rows in the specified database table.

**8. Run ETL Process**
<br>The run_etl method coordinates the entire ETL process: extraction, transformation, and loading of data.
    
**9. Main Function**
<br>The main function initializes the ETL process with the given parameters and runs it. It also prints and logs the number of rows in the database table after the ETL process.

<br>By following these steps, you can understand how the ETL process is implemented, from extracting data into batches, transforming the data with either Dask or concurrent futures, loading the transformed data into a SQLite database, and finally counting the rows in the database table to verify the process completion.
